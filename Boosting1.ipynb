{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d377d249-64c4-4c4a-aef7-9e58cf69119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ce7c0b-9775-4aae-9df8-7b78d330a910",
   "metadata": {},
   "outputs": [],
   "source": [
    " # machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db5c9b7f-3cbb-40de-8363-e6e52471c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011ede98-c199-479d-b455-aa2bee1d0bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benefits of boosting\n",
    "# The following are the top benefits of boosting:\n",
    "\n",
    "# It can use hyperparameter tuning options baked into many common algorithms.\n",
    "# It can reduce the bias of any one algorithm.\n",
    "# It can reduce the number of variables or dimensions required to make a decision or prediction, speeding computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d67f8ef8-8730-45ec-b415-53507f151b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawbacks of boosting\n",
    "# While boosting is a powerful machine learning tool that turns weak learners into strong learners, it does have some drawbacks:\n",
    "\n",
    "# In some cases, boosting can overfit data, making it hard to extend to new use cases.\n",
    "# The sequential nature of boosting makes it harder to scale or run for real-time analysis.\n",
    "# Accuracy can sometimes suffer with outliers outside of the norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b468fd4-a133-47b9-b5b5-ba535e93d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain how boosting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5bf48f9-6a62-4f21-9928-39930328b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting ensemble method\n",
    "# It assigns weights to the output of individual trees. Then it gives incorrect classifications from the first decision tree a\n",
    "# # higher weight and input to the next tree. \n",
    "# After numerous cycles, the boosting method combines these weak rules into a single powerful prediction rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b390a3-59fd-4a04-aaaf-780e8f34c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42abb7ce-d2d4-4968-a965-f430b7b4773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of boosting in machine learning\n",
    "# There are many types of boosting in machine learning. But three of the most popular include the following:\n",
    "\n",
    "# AdaBoost is an adaptive boosting technique in which the weights of data are adjusted based on the success of each (weak learner) algorithm and passed to the next weak learner to correct. An algorithm that missed a pug's nose in detecting dogs would emphasize the importance of using other features as identifiers for the next algorithm in the chain.\n",
    "# Gradient boosting is another popular technique in which new algorithms are dynamically crafted on the fly in response to the detection of errors in previous algorithms.\n",
    "# XGBoost trains an ensemble of algorithms at once and in parallel, and then the weights are adjusted and fed back to all of them collectively to improve the accuracy of the whole. Each algorithm is trained separately across multiple CPUs or GPUs, which reduces the training time and improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc267c4-b2f5-4289-9d94-e89ad0ccbed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Booster Parameters\n",
    "# Though there are 2 types of boosters, We’ll consider only tree booster here because it always outperforms the linear booster, and thus the latter is rarely used.\n",
    "\n",
    "# Parameter\tDescription\tTypical Values\n",
    "# eta\tAnalogous to the learning rate in GBM.\t0.01-0.2\n",
    "# min_child_weight\tDefines the minimum sum of weights of observations required in a child.\tTuned using CV\n",
    "# max_depth\tThe maximum depth of a tree. Used to control over-fitting.\t3-10\n",
    "# max_leaf_nodes\tThe maximum number of terminal nodes or leaves in a tree.\t\n",
    "# gamma\tSpecifies the minimum loss reduction required to make a split.\tTuned depending on loss function\n",
    "# max_delta_step\tAllows each tree’s weight estimation to be constrained.\tUsually not needed, explore if necessary\n",
    "# subsample\tDenotes the fraction of observations to be random samples for each tree.\t0.5-1\n",
    "# colsample_bytree\tDenotes the fraction of columns to be random samples for each tree.\t0.5-1\n",
    "# colsample_bylevel\tDenotes the subsample ratio of columns for each split in each level.\tUsually not used\n",
    "# lambda\tL2 regularization term on weights (analogous to Ridge regression).\tExplore for reducing overfitting\n",
    "# alpha\tL1 regularization term on weight (analogous to Lasso regression).\tUsed for high dimensionality\n",
    "# scale_pos_weight\tUsed in case of high-class imbalance for faster convergence.\t> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1522fb8-382c-40a5-b148-df369533dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7fb2e79-f330-450c-a541-cc340458c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, \n",
    "# each model tries to compensate for the weaknesses of its predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f1b3a2f-bfd5-4d21-92d2-3d5fe2673795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the concept of AdaBoost algorithm and its working.\n",
    "# AdaBoost is an adaptive boosting technique in which the weights of data are adjusted based on the success of each (weak learner) \n",
    "# algorithm and passed to the next weak learner to correct. An algorithm that missed a pug's nose in detecting dogs would emphasize \n",
    "# the importance of using other features as identifiers for the next algorithm in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106c2b9-8317-42a9-a7de-e642eebac7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the loss function used in AdaBoost algorithm?\n",
    "\n",
    "#It uses an exponential loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249ffcf-7977-4075-967b-367663028d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "# What this algorithm does is that it builds a model and gives equal weights to all the data points. It then assigns higher weights to\n",
    "# points that are wrongly classified. Now all the points with higher weights are given more importance in the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19940b5-d574-40de-aae4-a9f6b2d4bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b93336-e485-48a5-8ca0-81391ba2fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing the number of estimators may increase the complexity of the algorithm and may also lead to better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd1c4a-cc71-4e98-9576-d9feff9daab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
