{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04006e91-737e-4f1e-b4dc-8dcd7f417292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f462906-3980-489e-ae3f-4ffed328c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97b8a2c8-21f4-48e9-a343-56c57be5ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cec5cf83-5652-4486-bb67-f39a9ee31696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2\n",
    "# Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6f17c63-00a5-46ec-a0d8-db3e2c8ae06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6fb71ef-db9e-43a7-9a93-dcbea0107d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2643499-d2ad-431f-a2bf-a231a47ed25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4f6d25c-fd4e-48ac-972d-fc2c2adafcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In short, MAE evaluates the absolute distance of the observations (the entries of the dataset) to the predictions on a regression, taking the average over all observations. We use the absolute value of the distances so that negative errors are accounted properly. This is exactly the situation described on the image above.\n",
    "\n",
    "\n",
    "# Another way to do so is by squaring the distance, so that the results are positive. This is done by the MSE, and higher errors (or distances) weigh more in the metric than lower ones, due to the nature of the power function.\n",
    "\n",
    "\n",
    "# A backlash in MSE is the fact that the unit of the metric is also squared, so if the model tries to predict price in US$, the MSE will yield a number with unit (US$)² which does not make sense. RMSE is used then to return the MSE error to the original unit by taking the square root of it, while maintaining the property of penalizing higher errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54f195ab-dc46-4ec8-98ab-9d1f325e803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd5195-0fe6-4408-93b6-7f6b8667e8d8",
   "metadata": {},
   "source": [
    "\n",
    "Mean absolute Error (MAE)\n",
    "\n",
    "Mean square Error (MSE)\n",
    "\n",
    "Root mean square error (RMSE)\n",
    "\n",
    "Root mean square log Error (RMSLE)\n",
    "\n",
    "MAE doesn’t account for the direction of the value. Even if value is negative, positive value is used for calculation.\n",
    "\n",
    "MSE does account for positive or negative value.\n",
    "\n",
    "RMSE does account for positive or negative value.\n",
    "\n",
    "It does account for positive or negative value.\n",
    "\n",
    "RMSE & MSE share many properties with MSE because RMSE is simply the square root of MSE.\n",
    "\n",
    "RMSE & MSE share many properties with MSE because it is simply the square root of MSE.\n",
    "\n",
    "MAE is less biased for higher values. It may not adequately reflect the performance when dealing with large error values.\n",
    "\n",
    "MSE is highly biased for higher values.\n",
    "\n",
    "RMSE is better in terms of reflecting performance when dealing with large error values.\n",
    "\n",
    "RMSE is more useful when lower residual values are preferred.\n",
    "\n",
    "MAE is less than RMSE as the sample size goes up.\n",
    "\n",
    "RMSE tends to be higher than MAE as the sample size goes up.\n",
    "\n",
    "MAE doesn’t necessarily penalize large errors.\n",
    "\n",
    "MSE penalize large errors.\n",
    "\n",
    "RMSE penalize large errors.\n",
    "\n",
    "MAE is more useful when the overall impact is proportionate to the actual increase in error. For example- if error values go up to 6 from 3, actual impact on the result is twice. It is more common in financial industry where a loss of 6 would be twice of 3.\n",
    "\n",
    "RMSE is more useful when the overall impact is disproportionate to the actual increase in error. For example- if error values go up to 6 from 3, actual impact on the result is more than twice. This could be common in clinical trials, as error goes up, overall impact goes up disproportionately.\n",
    "\n",
    "RMSE -When actual and predicted values are low, RMSE & RMSLE are usually same.\n",
    "\n",
    "RMSE - When either of actual or predicted values are high, RMSE > RMSLE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f925196d-eb6f-44de-af25-9bde87ab6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e514dbc1-422d-42dc-9ffd-65bda48f5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso regression, commonly referred to as L1 regularization, is a method for stopping overfitting in linear regression models by including a penalty term in the cost function. In contrast to Ridge regression, it adds the total of the absolute values of the coefficients rather than the sum of the squared coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18d40235-833d-4180-b329-d4fb89f6ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2aa95d1a-2321-42cf-81a3-a86e58aa24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L2 regularization, also known as Ridge Regression, adds a penalty term proportional to the square of the model's parameters. This encourages the model to use all of the parameters but to reduce their values, resulting in a model that is less complex and less prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8496241-a64f-4740-94df-35b8ca3bec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2250de-43bc-49e5-bce6-45b10db594ac",
   "metadata": {},
   "source": [
    "1. Linearity: The assumption of linearity between variables restricts linear regressions. The premise of a straight-line relationship is usually false and may provide inaccurate results.\n",
    "2. Overfit: It's not recommended to use linear regressions when the observations aren't proportional to the features. Using it may produce extreme similarities between an analysis and a data set and may end up failing to produce reliable results and predict future observations accurately.\n",
    "3. Outliers: Linear regressions are prone to mistakes and outliers. They multiply the need for analysts to analyze and remove any anomaly before applying linear regression to the data.\n",
    "4. Multicollinearity: It's often crucial to remove any trace of multicollinearity because of its assumption that no relationship exists among the independent variables. You can remove them through dimensionality reduction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "188059e1-c4a1-4065-8f32-7e60343708ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q9\n",
    "#Comparing the two models: - Model A has a lower RMSE (10), indicating that it might perform better when large errors are of particular concern. - Model B has a lower MAE (8), suggesting that it might perform better in terms of overall average prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8983a7c4-6ec3-40a9-ae94-1281c3dd3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d98f9cac-86b0-4bd1-9e6e-3fb7cb35130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are comparing the performance of two regularized linear models using different types of\n",
    "# regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "# uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "# better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "# method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7172468-c852-4310-9ea2-a431916f2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # It stands for Least Absolute Shrinkage and Selection Operator\n",
    "# #Lasso\n",
    "# It adds L1 the penalty\n",
    "# L1 is the sum of the absolute value of the beta coefficients\n",
    "# Cost function = Loss + λ + Σ ||w||\n",
    "# Here,\n",
    "# Loss = sum of squared residual\n",
    "# λ = penalty\n",
    "# w = slope of the curve\n",
    "# What is Ridge Regularization (L2)\n",
    "# It adds L2 as the penalty\n",
    "# L2 is the sum of the square of the magnitude of beta coefficients\n",
    "# Cost function = Loss + λ + Σ ||w||2\n",
    "# Here,\n",
    "# Loss = sum of squared residual\n",
    "# λ = penalty\n",
    "# w = slope of the curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7cc8b97-85ec-485d-a0eb-7b54d40c23fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depending on my What our requirements are:\n",
    "#Ridge can be used to deal with Overfitting\n",
    "#Lasso can be used to do Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74255ef-0e3b-4351-be33-486b5b4f9221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
