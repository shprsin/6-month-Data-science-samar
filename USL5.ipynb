{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363b71bd-5605-4ab5-9e62-bcbea9b00b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa3f93e-980b-43f0-b143-7387a2d7f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, \n",
    "# where N is the total number of target classes. The matrix compares the actual target values with those predicted \n",
    "# by the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fef06d4-523e-4050-92a2-c93d2bc7af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5701f0-83ef-4334-9c67-5d48f3b67a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pair confusion matrix computes a 2 by 2 similarity matrix between two clusterings by considering all\n",
    "# pairs of samples and counting pairs that are assigned into the same or into different clusters under \n",
    "# he true and predicted clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e939f011-b7cb-40ac-a72f-b7c962f07e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c89c61b-0c9b-40eb-9d44-ac098f052760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrinsic evaluation is the best way to evaluate the performance of a language model by embedding it in an application and measuring how much the application improves.\n",
    "\n",
    "# It is an end-to-end evaluation where we can understand if a particular improvement in a component is really going to help the task at hand.\n",
    "# Example: For speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4d1642-2f1b-4ef4-a05b-675dad041b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acab72c9-cae6-4f7c-8f95-72b0f2eb3b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An intrinsic evaluation metric is one that measures the quality of a model-independent of any application.\n",
    "\n",
    "# We also need a test set for an intrinsic evaluation of a language model in NLP\n",
    "# The probabilities of an N-gram model training set come from the corpus it is trained on, the training set or training corpus.\n",
    "# We can then measure the quality of an N-gram model by its performance on some unseen test set data called the test set or test corpus.\n",
    "# We will also sometimes call test sets and other datasets that are not in our training sets held out corpora because we hold them out from the training data.\n",
    "# Good scores during intrinsic evaluation do not always mean better scores during extrinsic evaluation, so we need both types of evaluation in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d1d332-6bb5-4d6e-a701-070697471197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1745c4b-f150-4c75-8542-c9a4698f81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix is a table that is used to define the performance of a classification algorithm. \n",
    "# A confusion matrix visualizes and summarizes the performance of a classification algorithm.\n",
    "# Using confusion Matrix we can evaluate accuracy (ACC), precision (P), sensitivity (Sn), specificity (Sp), and F-score values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e8d8379-ec03-49da-8dc6-508bbee3b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a82e092-a6e3-4eeb-9484-b2117f5f9df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Silhouette Coefficient:\n",
    "\n",
    "# # It takes values between -1 and 1, where a value near to 1 demonstrates that the information focuses inside a cluster are \n",
    "# # firmly stuffed, and the clusters are well-separated from other clusters. A value near to -1 demonstrates that the information \n",
    "# # focuses are misclassified or the clusters are overlapping. It is calculated by comparing the average distance between a data\n",
    "# # point and all other points in its cluster.\n",
    "\n",
    "# Silhouette Coefficient (SC)= (b-a)/max(a,b)\n",
    "\n",
    "# The Calinski-Harabasz index measures the ratio of the between-cluster variance to the within-cluster variance. \n",
    "# It takes higher values for clusters that are well-separated and dense.\n",
    "\n",
    "# Calinski-Harabasz index = (Between-cluster variance) / (Within-cluster variance)\n",
    "\n",
    "# The Davies-Bouldin index measures the average similarity between each cluster and its most similar cluster, \n",
    "# relative to the average distance between each cluster and its most dissimilar cluster. \n",
    "# It takes lower values for clusters that are well-separated and dense. The formula for the DBI is as follows:\n",
    "#     DBI = (1/K) * Σmax(sim(c_i, c_j))\n",
    "\n",
    "# Mutual Information:\n",
    "\n",
    "# The mutual information measures the amount of information shared between the true labels and the predicted labels. \n",
    "# It takes values between 0 and 1, where a value close to 1 indicates that the predicted labels are identical to the true labels.\n",
    "# MI(X,Y) = ∑x∈X∑y∈Y p(x,y) log2 (p(x,y) / p(x)p(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7127b582-f856-48fd-a7e0-0f95f6c85171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66943e81-7af4-4cfb-b344-4298b1eab3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The limitation is:\n",
    "\n",
    "# Accuracy is biased when there is an imbalance of data. The accuracy calculation would favor the majority class as the model that keeps predicting the majority class would have great accuracy.\n",
    "\n",
    "# Accuracy took the false prediction as equal to each other, whether a false positive or false negative. In reality, some misclassification cost more than others. For example, a false negative in cancer prediction is more dangerous than a false positive.\n",
    "\n",
    "# Accuracy is not a good representation for models with multiclass labels. Similar to the imbalance cases, a model that consistently predict a majority label on the multiclass would always have a good score.\n",
    "\n",
    "# That is why there are still various metrics to evaluate our models, such as Precision, Recall, or F1 score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
