{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4acf14-f3b0-4293-a8dc-7947db48ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "\n",
    "# It's not fast, but it will build nonlinear models and perform selection. To directly answer your question, LASSO is a linear method, but you could make nonlinear terms through transformation (e.g. Lag1^2) and see if they stay in the model.\n",
    "\n",
    "#Q6\n",
    "\n",
    "# Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization.\n",
    "\n",
    "#Q7\n",
    "\n",
    "# Another Tolerant Method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator (LASSO) regression, solves the same constrained optimization problem as ridge regression, but uses the L1 norm rather than the L2 norm as a measure of complexity\n",
    "\n",
    "#Q8\n",
    "\n",
    "# The first step to build a lasso model is to find the optimal lambda value using the code below. For lasso regression, the alpha value is 1. The output is the best cross-validated lambda, which comes out to be 0.001. Once we have the optimal lambda value, we train the lasso model in the first line of code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacd5800-e282-4a26-b415-4f518d4e4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d08d173-6ff9-4ef7-91a7-08e1467b436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b197c32-eb3d-4353-8f8d-ce8843df3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c819d879-4fe2-4ede-ad8e-6b1dfbb2308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages:\n",
    "\n",
    "# Feature selection: Lasso helps identify the most important features, making the model more interpretable.\n",
    "\n",
    "# Reduces overfitting: By adding a penalty term, Lasso reduces the risk of overfitting on the training data.\n",
    "\n",
    "# Works well with high-dimensional data: Lasso is particularly effective when dealing with datasets with many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea228c9b-336d-4404-8627-d404bb5942d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a9ae9e-50ac-47b7-8523-64e62747103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso can be interpreted as linear regression for which the coefficients have Laplace prior distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d9c9e-9a72-447f-a0a6-c86f049f5413",
   "metadata": {},
   "source": [
    "#q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08820a57-4c04-45ed-bac1-6f4cdc34aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf06ec76-bb70-42c3-81aa-12b910ed154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As tuning paramaeter λ rises the Slope decreases and the minima starts shifting to Zero on X axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d3a36a2-fb4a-45cb-a8c8-0524b0a1ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "\n",
    "# It's not fast, but it will build nonlinear models and perform selection. To directly answer your question, LASSO is a linear method, but you could make nonlinear terms through transformation (e.g. Lag1^2) and see if they stay in the model.\n",
    "\n",
    "#Q6\n",
    "\n",
    "# Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization.\n",
    "\n",
    "#Q7\n",
    "\n",
    "# Another Tolerant Method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator (LASSO) regression, solves the same constrained optimization problem as ridge regression, but uses the L1 norm rather than the L2 norm as a measure of complexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e4c3d5-0bd0-4b69-be58-7bc179490f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8\n",
    "\n",
    "# We choose the parmaeters by conducting a Cross Validation in n-Fold way depending on requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeba510-ea23-4074-b0db-293126a11048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
