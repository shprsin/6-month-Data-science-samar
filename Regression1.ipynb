{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d157b5-dfe6-4242-a353-248ecdbe6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db953a63-4574-444e-9773-716fe654a53c",
   "metadata": {},
   "source": [
    "# Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables. For instance, when we predict rent based on square feet alone that is simple linear regression.\n",
    "# Example\n",
    "# Simple Linear Regression: \n",
    "1. Dependent Feature: Weight\n",
    "2. Independent Feature: Height\n",
    "# Multiple Regression:\n",
    "1. Dependent Feature: Price\n",
    "2. Independent Feature:Location, Area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5c4fb4-b469-4832-a67c-ebebc5998bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60c8351-994d-46d9-9714-5bc5773cd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumptions are:\n",
    "# There is a linear relationship between the predictors (x) and the outcome (y)\n",
    "# Predictors (x) are independent and observed with negligible error.\n",
    "# Residual Errors have a mean value of zero.\n",
    "# Residual Errors have constant variance.\n",
    "# Residual Errors are independent from each other and predictors (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c064061-844a-4812-8b0e-1407beec7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test the linearity assumption using Python\n",
    "# This can be done in two ways:\n",
    "\n",
    "# An easy way is to plot y against each explanatory variable x_j and visually inspect the scatter plot for signs of non-linearity.\n",
    "# One could also use the DataFrame.corr() method in Pandas to get the Pearson’s correlation coefficient ‘r’ between the response variable y and each explanatory variable x_j to get a quantitative feel for the degree of linear correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905e5fdf-b549-4445-9dee-f9092270e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86511616-55f9-4fa6-80ca-d5016d985b0c",
   "metadata": {},
   "source": [
    "# Example \n",
    "- Data were collected on the depth of a dive of penguins and the duration of the dive. The following linear model is a fairly good summary of the data, where t is the duration of the dive in minutes and d is the depth of the dive in yards. The equation for the model is d = + 0.015 2.915t \n",
    "1. Interpret the slope: If the duration of the dive increases by 1 minute, we predict the depth of the dive will increase by approximately 2.915 yards.\n",
    "2. Interpret the intercept. If the duration of the dive is    0 seconds, then we predict the depth of the dive is 0.015 yards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ae1161-9da2-4c6e-8881-9a69bd0f3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe3963cf-49a9-4f49-bd13-e461474bda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent (GD) is an iterative first-order optimisation algorithm, used to find a local minimum/maximum of a given function. This method is commonly used in machine learning (ML) and deep learning (DL) to minimise a cost/loss function (e.g. in a linear regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e236d3a-94a3-41ca-b879-1fbb7f018d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "590919be-67d6-47c8-b42a-f3cc65beaed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables. Whereas linear regress only has one independent variable impacting the slope of the relationship, multiple regression incorporates multiple independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94cca459-1808-4389-9322-be96b5f6d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "969eb3c6-10ae-42be-8ee1-9d0ef693e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity exists whenever an independent variable is highly correlated with one or more of the other independent variables in a multiple regression equation. Multicollinearity is a problem because it will make the statistical inferences less reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62ff51a5-bfc8-4a9f-8740-432d6473ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way to address multicollinearity is to center the predictors, that is substract the mean of one series from each value. Ridge regression can also be used when data is highly collinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d165ede-0796-40e6-9650-34c58f5c30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "978bed72-29f6-42f2-adfa-8be5dc7e35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the complexity of the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eed0201-61e1-432b-8aa9-5cfa7d77fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8\n",
    "#Advantages of Polynomial over Linear Regression are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1a569-5d18-44bc-bb6c-a52c6011e400",
   "metadata": {},
   "source": [
    "1) Polynomial regression is independent of the size of the data set.\n",
    "2) Non-linear problems are solved with good accuracy. \n",
    "3) It gives the best approximation of the correspondence between the output and explanatory (independent) variables. \n",
    "4) A large number of functions can be fit under it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9021b32c-9951-47a7-9cbf-27899553bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For cases where the data points are arranged in a non-linear fashion, there is a need for polynomial regression. If a non-linear model is present and you try to cover it using a linear model, it will cover no data points. Hence, a polynomial model is used to ensure that the data points are covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f107ab-d496-4d9d-b282-d417b040f395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
