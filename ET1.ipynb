{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b448e3-b71c-428f-bbc4-12b7eeacafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is an ensemble technique in machine learning?\n",
    "# Ensemble techniques in machine learning involve combining multiple individual models to create a stronger,\n",
    "# more robust model. The idea is to leverage the diversity among individual models to improve overall \n",
    "# predictive performance. The two main types of ensemble techniques are bagging and boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f1a3df-ac4a-406d-87c1-7a73bd44979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Why are ensemble techniques used in machine learning?\n",
    "# Ensemble techniques are used in machine learning for several reasons:\n",
    "\n",
    "# Improved Accuracy: Combining predictions from multiple models often leads to better accuracy compared to individual models.\n",
    "# Robustness: Ensembles are less prone to overfitting and can handle noisy data more effectively.\n",
    "# Stability: Ensembles are more stable, as they can compensate for the weaknesses of individual models.\n",
    "# Versatility: Ensemble methods can be applied to various types of models, making them versatile across different machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52fede17-3ed6-4945-87e5-0401f4900bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is bagging?\n",
    "# Bagging (Bootstrap Aggregating): Bagging is an ensemble technique where multiple instances of the same\n",
    "# learning algorithm are trained on different random subsets of the training data. These subsets are created \n",
    "# by sampling with replacement (bootstrap sampling). The final prediction is typically an average (for regression) or a majority vote (for classification) of the predictions made by individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1964c6c-56c2-4dcc-9cb1-ac243fa482ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is boosting?\n",
    "# Boosting: Boosting is another ensemble technique where weak learners (models that perform slightly better\n",
    "# than random chance) are combined to form a strong learner. Unlike bagging, boosting assigns weights to\n",
    "# training instances and adjusts them during the training process to give more importance to misclassified\n",
    "# instances. Boosted models are trained sequentially, and each subsequent model corrects the errors of the \n",
    "# previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ee09c7-cca9-4bb8-af93-01807ccc6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What are the benefits of using ensemble techniques?\n",
    "# Improved Performance: Ensembles often outperform individual models, leading to better predictive accuracy.\n",
    "# Robustness: Ensembles are more robust to noise and outliers in the data.\n",
    "# Generalization: They enhance the generalization ability of models by reducing overfitting.\n",
    "# Versatility: Ensemble methods can be applied to various types of models.\n",
    "# Model Interpretability: Ensembles can provide insights into the importance of different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654c1122-56c3-4d87-a021-51c4de553186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Are ensemble techniques always better than individual models?\n",
    "# While ensemble techniques generally improve performance, they may not always be better than individual models.\n",
    "# It depends on factors such as the quality of the base models, the diversity among them, and the nature of the \n",
    "# data. In some cases, a well-tuned individual model might perform as well as or better than an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70fb4d71-c40a-4c5a-8b09-2f7e760fa77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How is the confidence interval calculated using bootstrap?\n",
    "# A confidence interval using bootstrap involves resampling the dataset with replacement to create multiple \n",
    "# bootstrap samples. The confidence interval is then constructed from the distribution of a statistic (e.g.,\n",
    "# mean, median) calculated on these bootstrap samples. The interval is typically defined by percentiles of \n",
    "# the distribution (e.g., 2.5th and 97.5th percentiles for a 95% confidence interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e948d2-d6c2-40da-8d3a-e48055a81ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. How does bootstrap work, and what are the steps involved in bootstrap?\n",
    "# Bootstrap:\n",
    "\n",
    "# Step 1: Randomly sample data points with replacement from the original dataset to create a bootstrap sample.\n",
    "# Step 2: Perform the analysis or computation of interest on the bootstrap sample.\n",
    "# Step 3: Repeat steps 1 and 2 a large number of times (e.g., thousands of times) to create multiple bootstrap samples.\n",
    "# Step 4: Calculate the desired statistic (e.g., mean, median) for each bootstrap sample.\n",
    "# Step 5: Construct the confidence interval using percentiles of the distribution of the calculated statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f55124b1-04e8-4644-982e-dea1014a9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "933662c3-ef59-435d-967f-df3fd207ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given data\n",
    "sample_mean = 15  # mean height of the sample\n",
    "sample_std = 2    # standard deviation of the sample\n",
    "sample_size = 50  # size of the sample\n",
    "num_bootstrap_samples = 10000  # number of bootstrap samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502fa862-0c05-4fb0-8c25-59f8df258f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for the Population Mean Height: [14.43788035 15.55985683]\n"
     ]
    }
   ],
   "source": [
    "# Generate bootstrap samples\n",
    "bootstrap_samples = np.random.normal(loc=sample_mean, scale=sample_std, size=(num_bootstrap_samples, sample_size))\n",
    "\n",
    "# Calculate the mean height for each bootstrap sample\n",
    "bootstrap_sample_means = np.mean(bootstrap_samples, axis=1)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_sample_means, [2.5, 97.5])\n",
    "\n",
    "# Display the result\n",
    "print(f\"95% Confidence Interval for the Population Mean Height: {confidence_interval}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf6207-a423-4f77-99c3-66c0004f691d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
