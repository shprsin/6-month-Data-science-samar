{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9bd958f8-8f76-4fe0-bff2-b051ce3b09e1",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cd42d7a-a86e-4e2b-8149-12fa6da4bb55",
   "metadata": {},
   "source": [
    "Overfitting models produce good predictions for data points in the training set but perform poorly on new samples. Underfitting occurs when the machine learning model is not well-tuned to the training set. The resulting model is not capturing the relationship between input and output well enough.\n",
    "\n",
    "Overfitting leads to low bias and high variance.\n",
    "Underfitting leads to high bias and high Variance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7393e14-d9d9-429c-8a95-92800c6f4f77",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77f149e5-3c41-4169-892e-4c15b162afe5",
   "metadata": {},
   "source": [
    "We can prevent overfitting by diversifying and scaling our training data set or using startegies such as :\n",
    "\n",
    "Pruning:pruning—identifies the most important features within the training set and eliminates irrelevant ones\n",
    "\n",
    "Regularization:hese methods try to eliminate those factors that do not impact the prediction outcomes by grading features based on importance\n",
    "\n",
    "Ensemble: Ensemble methods combine all the weak learners to get more accurate results. They use multiple models to analyze sample data and pick the most accurate outcomes. The two main ensemble methods are bagging and boosting. Boosting trains different machine learning models one after another to get the final result, while bagging trains them in parallel.\n",
    "\n",
    "Data augmentation:Data augmentation is a machine learning technique that changes the sample data slightly every time the model processes it."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ffa7872-11ca-415c-a588-3e878587d189",
   "metadata": {},
   "source": [
    "Q3\n",
    "Underfitting is another type of error that occurs when the model cannot determine a meaningful relationship between the input and output data. You get underfit models if they have not trained for the appropriate length of time on a large number of data points.\n",
    "\n",
    "Underfitting can occur when:\n",
    "1.Data used for training is not cleaned and contains noise (garbage values) in it.\n",
    "2.The model has a high bias.\n",
    "3.The size of the training dataset used is not enough.\n",
    "4.The model is too simple."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d5393f6-04aa-448f-bc4a-5905aef9e566",
   "metadata": {},
   "source": [
    "Q4\n",
    "The bias is known as the difference between the prediction of the values by the Machine Learning model and the correct value.\n",
    "The variability of model prediction for a given data point which tells us the spread of our data is called the variance of the model.\n",
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as a Trade-off or Bias Variance Trade-off.\n",
    "Total Error= Bias^2 + Variance + Irreducible Error\n",
    "So we try to optimise the above equation for generalized mode l zone."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e748daf-db90-49d9-bfa9-fb0093febd32",
   "metadata": {},
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2b6d75d-fff8-41fb-a212-29d0fc3076ea",
   "metadata": {},
   "source": [
    "Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.\n",
    "\n",
    "Poor performance on the training data could be because the model is too simple (the input features are not expressive enough) to describe the target well. Performance can be improved by increasing model flexibility. To increase model flexibility, try the following:\n",
    "\n",
    "Add new domain-specific features and more feature Cartesian products, and change the types of feature processing used (e.g., increasing n-grams size)\n",
    "\n",
    "Decrease the amount of regularization used\n",
    "\n",
    "If your model is overfitting the training data, it makes sense to take actions that reduce model flexibility. To reduce model flexibility, try the following:\n",
    "\n",
    "Feature selection: consider using fewer feature combinations, decrease n-grams size, and decrease the number of numeric attribute bins.\n",
    "\n",
    "Increase the amount of regularization used.\n",
    "\n",
    "Accuracy on training and test data could be poor because the learning algorithm did not have enough data to learn from. You could improve performance by doing the following:\n",
    "\n",
    "Increase the amount of training data examples.\n",
    "\n",
    "Increase the number of passes on the existing training data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "11d47220-498e-4e33-a0e7-277e89a58b86",
   "metadata": {},
   "source": [
    "Q6\n",
    "Bias \tVariance \n",
    "Bias occurs in a machine learning model when an algorithm is used but does not fit properly. \t\n",
    "Variance is the amount of variation the target function estimation will change if different training data is used.\n",
    "Bias is the difference between the actual values and the predicted values. \t\n",
    "Variance talks about how much any random variable deviated from the expected value. \n",
    "For Bias The model cannot find patterns in the training dataset, failing for unseen and seen data. \n",
    "For Variance The model can find most patterns from the dataset. It learns from noise or unnecessary data. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5642bee4-0ece-4c02-9668-8bf623f0afb7",
   "metadata": {},
   "source": [
    "Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance.\n",
    "\n",
    "When a data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance. This way, the model will fit with the data set while increasing the chances of inaccurate predictions.\n",
    "\n",
    "The same applies when creating a low variance model with a higher bias. While it will reduce the risk of inaccurate predictions, the model will not properly match the data set."
   ]
  },
  {
   "cell_type": "raw",
   "id": "663e4838-0857-4a2b-a84b-822a546cb757",
   "metadata": {},
   "source": [
    "Q7\n",
    "Regularization:These methods try to eliminate those factors that do not impact the prediction outcomes by grading features based on importance.\n",
    "Some Common Methods are:\n",
    "1. Modify loss function\n",
    "In these regularization techniques, the loss function under which the model is optimized is modified to directly take into account the norm of the learned parameters or the output distribution. We have the following loss function based regularization techniques.\n",
    "\n",
    "a. L2 Regularization (strong):In L2 regularization, we modify the loss to include the weighted L2 norm of the weights (beta) being optimized. This prevents the weights from getting too large and hence avoiding them to overfit\n",
    "\n",
    "b. L1 Regularization (strong):\n",
    "Instead of using the L2 norm of the weights in the loss function, in L1 regularization, the L1 norm (absolute values) of the weights are used\n",
    "\n",
    "c.Entropy Regularization (strong):\n",
    "Entropy quantifies the probability distribution in terms of uncertainty in them. Greater the uncertainty in the distribution, the greater the entropy. A uniform distribution has an equal probability of all its event occurring, which means the amount of uncertainty is maximum and hence maximum entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0031c-dff3-4732-9e4e-e2a38970d183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
