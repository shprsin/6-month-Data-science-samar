{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820c0150-b348-44ef-bdbe-c3d52baa4d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fba619b2-be07-4c3d-a1e7-0c23d96c6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Hierarchical clustering method works via grouping data into a tree of clusters. Hierarchical clustering begins by treating every data point as a separate cluster. Then, it repeatedly executes the subsequent steps:\n",
    "\n",
    "# Identify the 2 clusters which can be closest together, and\n",
    "# Merge the 2 maximum comparable clusters. We need to continue these steps until all the clusters are merged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a252790-68ed-471c-ba36-bd4f7496917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7581887d-83f7-4e60-a91d-96412dec485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative Clustering\n",
    "# Divisive clustering\n",
    "# 1. Agglomerative Clustering\n",
    "# Initially consider every data point as an individual Cluster and at every step, merge the nearest pairs of the cluster. (It is a bottom-up method). At first, every dataset is considered an individual entity or cluster. At every iteration, the clusters merge with different clusters until one cluster is formed. \n",
    "\n",
    "# The algorithm for Agglomerative Hierarchical Clustering is:\n",
    "# Calculate the similarity of one cluster with all the other clusters (calculate proximity matrix)\n",
    "# Consider every data point as an individual cluster\n",
    "# Merge the clusters which are highly similar or close to each other.\n",
    "# Recalculate the proximity matrix for each cluster\n",
    "# Repeat Steps 3 and 4 until only a single cluster remains.\n",
    "\n",
    "# 2. Divisive Hierarchical clustering\n",
    "# Divisive Hierarchical clustering is precisely the opposite of Agglomerative Hierarchical clustering.\n",
    "# In Divisive Hierarchical clustering, we take into account all of the data points as a single cluster and in every iteration,\n",
    "# we separate the data points from the clusters which arenâ€™t comparable. In the end, we are left with N clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee4658f-27b9-4623-a6a0-9fb487fa80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01af618f-a01f-4a33-9e4a-ade4bb5ed17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we do is Calculate the similarity of one cluster with all the other clusters (calculate proximity matrix). This distance\n",
    "# Can be calculated using Eucledian Distance or Manhattan Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d465e1a-98cf-483b-b15b-6eda1698220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb8f7ee6-f2de-4673-91ea-53b00512a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the optimal number of clusters for hierarchical clustering, we make use a dendrogram which is tree-like \n",
    "# chart that shows the sequences of merges or splits of clusters. If two clusters are merged, the dendrogram will join\n",
    "# them in a graph and the height of the join will be the distance between those clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c5047e-2dc3-43df-b238-60b8113ef191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5\n",
    "# Dendrogram is a diagram that shows the hierarchical relationship between objects.\n",
    "# It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram\n",
    "# is to work out the best way to allocate objects to clusters as the height of the join will be the distance between those clusters.\n",
    "# By finding the line which has no horizontal line passing through we can select the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "747a013c-9303-4972-b5cb-c71845b22839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e50523e6-276c-4c0a-a93a-de917f1adb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes it can be used with both categorical and continuous data. For categorical data we can calculate cosine distance and for continuous\n",
    "# we may use Eucledian/Manhattan Distance to calculate clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d78cb11-ce15-409c-b24f-a8d016507809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6589cd72-ca95-4e3b-a7fd-71f9f756a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers can be detected by using the following methods: Distance from centroid: After performing clustering,\n",
    "# each data point will belong to a cluster with a defined centroid. Data points far away from the centroid of their\n",
    "# cluster can be considered potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981437f-bdfc-4978-955e-25b6ecda9015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
