{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25990fc8-e653-47ad-bbb7-6fd07cfb4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6cea67e-3771-4f0e-9351-70eca87305c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression works with an enhanced cost function when compared to the least squares cost function. Instead of the simple sum of squares, Ridge regression introduces an additional 'regularization' parameter that penalizes the size of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e844a42-8b5e-4109-abbe-2c34c234b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df13c790-9df3-4f2a-9193-3a921ffa2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The assumptions are the same as those used in regular multiple regression: linearity, constant variance (no outliers), and independence. Since ridge regression does not provide confidence limits, normality need not be assumed. Ridge regression remains controversial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215d0e12-a2ed-4dcc-ada4-5b0c38217da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chosen sample is representative of the population.\n",
    "# There is a linear relationship between the independent variable(s) and the dependent variable.\n",
    "# All the variables are normally distributed; to check, plot a histogram of the residuals.\n",
    "# There are no outliers, (if there are outliers they need to be removed); to check use a test for detecting outliers.\n",
    "# The independent variables are all linearly independent (no variable dependents of the other variables); to check plot the independent values against each other and look for a correlation.\n",
    "# For multiple regression there should be at least five times as many pairs of data than dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3f8a279-effe-4c24-ac94-b64e34156ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6332d9f-3091-404c-b97c-c3a5ee99d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default Value of Alpha is 1.\n",
    "# Instead of arbitrarily choosing λ=4, it would be better to use cross-validation to choose the tuning parameter λ. We can do this using the built-in cross-validation function, cv. glmnet() . By default, the function performs 10-fold cross-validation, though this can be changed using the argument folds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540884d7-9d33-424a-a130-c87ca9e603db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d56072f-c2bc-421d-b41b-2e5537510830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could see ridge regression as doing the feature 'selection' in a nuanced way by reducing the size of the coefficients instead of setting them equal to zero. You could elliminate the features with the smaller coefficients*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1529a88f-6424-49cd-9b16-2490b03332f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf812318-0939-4e02-b8c6-4a4e9be4cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's not fast, but it will build nonlinear models and perform selection. To directly answer your question, LASSO is a linear method, but you could make nonlinear terms through transformation (e.g. Lag1^2) and see if they stay in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b97352-04f6-4b91-8cee-c862649775ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31defc86-5cb0-4a82-9621-008033c3f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "470b75ef-c403-4c86-a307-54394ab75fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf2908ed-8a07-43d8-ab8a-a0c23f78b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another Tolerant Method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator (LASSO) regression, solves the same constrained optimization problem as ridge regression, but uses the L1 norm rather than the L2 norm as a measure of complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac62fd84-2870-4218-a828-0d23325f4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ba038f7-8330-4566-b977-9396202edb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step to build a lasso model is to find the optimal lambda value using the code below. For lasso regression, the alpha value is 1. The output is the best cross-validated lambda, which comes out to be 0.001. Once we have the optimal lambda value, we train the lasso model in the first line of code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e735a8f-ffac-418d-a4ac-c9044bb092c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
