{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5bf53f-de3b-49f6-909b-8c43e1090daa",
   "metadata": {},
   "source": [
    "#q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e032c0a5-447c-418e-9b71-8963439b3fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that an employee is a smoker given that they use the health insurance plan is: 0.40\n"
     ]
    }
   ],
   "source": [
    "# Given probabilities\n",
    "P_Uses_Health_Insurance = 0.70  # Probability that an employee uses the health insurance plan\n",
    "P_Smoker_Given_Uses_Health_Insurance = 0.40  # Probability that an employee is a smoker given that they use the health insurance plan\n",
    "\n",
    "# Calculate the probability using the conditional probability formula\n",
    "P_Smoker_Given_Uses_Health_Insurance_calculated = P_Smoker_Given_Uses_Health_Insurance * P_Uses_Health_Insurance / P_Uses_Health_Insurance\n",
    "\n",
    "# Display the result\n",
    "print(f\"The probability that an employee is a smoker given that they use the health insurance plan is: {P_Smoker_Given_Uses_Health_Insurance_calculated:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e012862b-6f75-491d-9682-481525d0ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q2\n",
    "# Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm used for text classification and other related tasks. The main difference between them lies in the type of data they are designed to handle.\n",
    "\n",
    "# Bernoulli Naive Bayes:\n",
    "\n",
    "# Use Case: It is suitable for binary and sparse data, particularly for text classification tasks where the features are binary (e.g., word presence or absence).\n",
    "# Assumption: It assumes that features are binary, representing whether a particular term occurs in the document or not.\n",
    "# Example Application: Spam filtering, sentiment analysis (where the focus is on whether certain words are present in a document).\n",
    "# Multinomial Naive Bayes:\n",
    "\n",
    "# Use Case: It is suitable for discrete data, often used for text classification where features represent word counts or term frequencies (i.e., the number of times a term appears in a document).\n",
    "# Assumption: It assumes that features are multinomially distributed, which means they represent the frequency of terms in the document.\n",
    "# Example Application: Document classification, topic modeling, and spam filtering where the frequency of words matters.\n",
    "# In summary, the key distinction is in the type of data they are designed for: Bernoulli Naive Bayes is suitable for binary data, whereas Multinomial Naive Bayes is designed for discrete count data. Both variants assume independence between features given the class label, and they are called \"naive\" because of this assumption, even though it may not always hold in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2446bb20-9f51-4f80-87a6-73c81b1b89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f467453-1c3e-43f7-8dfb-7a07c220b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Bernoulli Naive Bayes, which is commonly used for binary and sparse data, the handling of missing values depends on the representation of the data and the implementation details.\n",
    "\n",
    "# When using Bernoulli Naive Bayes for text classification, for example, where the features are binary (indicating the presence or absence of a term), missing values can be treated in the following ways:\n",
    "\n",
    "# Ignore Missing Values:\n",
    "\n",
    "# If a term is missing (not present in the document), you can simply ignore it. The absence of a term is implicitly considered as a binary feature with a value of 0.\n",
    "# Imputation:\n",
    "\n",
    "# Alternatively, you may choose to impute missing values by assigning a default value. This could be 0, assuming that the missing value corresponds to the absence of the term. However, this approach may introduce bias if the missing values are not missing completely at random.\n",
    "# Special Value for Missing:\n",
    "\n",
    "# You can designate a special value (other than 0 and 1) to represent missing values. This way, the absence of a term is explicitly encoded as a separate category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b168097-0c78-4767-9857-1891a069f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44548d36-62a3-4c39-a079-4d71c9201e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Naive Bayes is adapted for multi-class classification:\n",
    "\n",
    "# Estimation of Parameters:\n",
    "\n",
    "# For each class, the algorithm estimates the mean and variance of each feature assuming a Gaussian distribution. This involves calculating the mean and variance for each feature separately for each class.\n",
    "# Class Prior Probability:\n",
    "\n",
    "# Additionally, the algorithm calculates the prior probability of each class, which is the probability of a document belonging to a particular class without considering the feature values.\n",
    "# Naive Bayes Formula:\n",
    "\n",
    "# During prediction, the Naive Bayes formula is used to calculate the posterior probability of each class given the observed feature values. The class with the highest posterior probability is predicted as the final class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e87d5339-c3e4-4968-8246-71f7bbbe4ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d25b945-5a60-4f33-aef6-df3628fb85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5223fb-06f6-427d-b18e-619aa3a29937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [f\"X{i}\" for i in range(1, 58)] + [\"spam\"]\n",
    "data = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data[\"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "266b4491-ee2f-4f06-9ba9-a9553a9c4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7740595a-30b6-4d6f-9402-87aedbe68fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = {\n",
    "    'Accuracy': make_scorer(accuracy_score),\n",
    "    'Precision': make_scorer(precision_score),\n",
    "    'Recall': make_scorer(recall_score),\n",
    "    'F1 Score': make_scorer(f1_score),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "377b4712-410d-4862-97be-bb6ea9f0c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BernoulliNB:\n",
      "Accuracy: 0.8839\n",
      "Precision: 0.8870\n",
      "Recall: 0.8152\n",
      "F1 Score: 0.8481\n",
      "\n",
      "MultinomialNB:\n",
      "Accuracy: 0.7863\n",
      "Precision: 0.7393\n",
      "Recall: 0.7215\n",
      "F1 Score: 0.7283\n",
      "\n",
      "GaussianNB:\n",
      "Accuracy: 0.8218\n",
      "Precision: 0.7104\n",
      "Recall: 0.9570\n",
      "F1 Score: 0.8131\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each classifier using 10-fold cross-validation\n",
    "classifiers = [bernoulli_nb, multinomial_nb, gaussian_nb]\n",
    "for classifier in classifiers:\n",
    "    print(f\"\\n{classifier.__class__.__name__}:\")\n",
    "\n",
    "    # Calculate and print performance metrics\n",
    "    for metric_name, metric_func in metrics.items():\n",
    "        scores = cross_val_score(classifier, X, y, cv=10, scoring=metric_func)\n",
    "        print(f\"{metric_name}: {np.mean(scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed61f93-7e4c-4ac2-8c05-1b087f412bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
