{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25990fc8-e653-47ad-bbb7-6fd07cfb4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6cea67e-3771-4f0e-9351-70eca87305c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression works with an enhanced cost function when compared to the least squares cost function. Instead of the simple sum of squares, Ridge regression introduces an additional 'regularization' parameter that penalizes the size of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e844a42-8b5e-4109-abbe-2c34c234b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df13c790-9df3-4f2a-9193-3a921ffa2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The assumptions are the same as those used in regular multiple regression: linearity, constant variance (no outliers), and independence. Since ridge regression does not provide confidence limits, normality need not be assumed. Ridge regression remains controversial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215d0e12-a2ed-4dcc-ada4-5b0c38217da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chosen sample is representative of the population.\n",
    "# There is a linear relationship between the independent variable(s) and the dependent variable.\n",
    "# All the variables are normally distributed; to check, plot a histogram of the residuals.\n",
    "# There are no outliers, (if there are outliers they need to be removed); to check use a test for detecting outliers.\n",
    "# The independent variables are all linearly independent (no variable dependents of the other variables); to check plot the independent values against each other and look for a correlation.\n",
    "# For multiple regression there should be at least five times as many pairs of data than dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3f8a279-effe-4c24-ac94-b64e34156ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6332d9f-3091-404c-b97c-c3a5ee99d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default Value of Alpha is 1.\n",
    "# Instead of arbitrarily choosing λ=4, it would be better to use cross-validation to choose the tuning parameter λ. We can do this using the built-in cross-validation function, cv. glmnet() . By default, the function performs 10-fold cross-validation, though this can be changed using the argument folds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540884d7-9d33-424a-a130-c87ca9e603db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d56072f-c2bc-421d-b41b-2e5537510830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could see ridge regression as doing the feature 'selection' in a nuanced way by reducing the size of the coefficients instead of setting them equal to zero. You could elliminate the features with the smaller coefficients*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af17841f-f4e3-4914-9ddc-75ceb7c0104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "#When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e735a8f-ffac-418d-a4ac-c9044bb092c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52174437-8d31-42cc-8184-ce2f3e7584cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, If you have categorical data (or mixed numeric and categorical) and you are going to create a linear ridge regression model, you can use one-hot encoding on categorical variables that have three or more possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d54c8208-bef1-4ef8-89a4-3ecde88ec533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beff6989-b7e1-4a1e-af6b-83c55a0e5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad04203c-7b70-4ea4-b274-ae497e749087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaeff50d-f958-4cec-bf5f-60f69f8ea8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The ridge regression technique can be used to predict time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08a01fa-64e1-47a3-94b2-03f7c711ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression adds a penalty term to the sum of squared errors of the linear regression model, which shrinks the coefficients of the predictors towards zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc230aff-4003-475c-bbb1-a890e8181b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RR can decrease the Multicollinearity and work on Time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e3049-4276-459b-96c0-922db04b00c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
